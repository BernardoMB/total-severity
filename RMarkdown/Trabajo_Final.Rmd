---
title: "Trabajo final de Cálculo Actuarial III"
author: "Bernardo Mondragón Brozon, Anhara Gámez Rueda"
date: "May 12, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

<!-- Configure R Markdown file -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
# Create beautiful graphs
#install.packages("ggplot")
library(ggplot2)
# TeX expressions in graphs labels
#install.packages("latex2exp")
library(latex2exp)
# Calculate Kurtosis skew and other functions
#install.packages("moments")
library(moments)
# Para knitear
#install.packages("knitr")
library(knitr)
# Para imprimir tablas
#install.packages("kableExtra")
library(kableExtra)

#install.packages("psych")
#library(psych)

# Color?
colorImp <- TRUE
# Colores chidos
moradito <- "#be58a0"
azulito <- "#00e6e6"
```

\section{El modelo colectivo}

\subsection{1.1 La severidad total}

La severidad total esta dada por lo siguiente:
$$ S=x_1+x_2+\cdots+x_N  $$
```{r, parameters, include=FALSE}
set.seed(7031996)
lambda_po <- 0.1
denom <- 10000 # Por la parametrizacion de la exponencial
lambda_exp <- 1/denom
```
En donde $\{x_i\}_{i=1\dots N}$ son variables aleatorias independientes e identicamente distribuidas que indican los montos tal que $x_i$ sigue una distribución exponencial con media $1/\lambda_{exp}=`r denom`$ para toda $i$, y $N$ es la variable aleatoria independiente de las $x_i$'s que indica la frecuencia de los montos tal que sigue una distribución Poisson con media $\lambda_{Po}=`r lambda_po`$ eventos en un año.

Por el teorema de probabilidad total, se tiene que 
$$Pr\{S\leq s|N=n\}=\frac{Pr\{S\leq s,N=n\}}{Pr\{N=n\}}  $$
$$\Rightarrow \quad Pr\{S\leq s,N=n\}=Pr\{N=n\}Pr\{S\leq s|N=n\}.$$
Por lo tanto se tiene que 
$$\begin{aligned}
  F_S(s) &= \sum_{n=0}^{\infty}Pr\{S\leq s,N=n\}  \\
            &= \sum_{n=0}^{\infty}Pr\{N=n\}Pr\{S\leq s|N=n\} \\
            &= \sum_{n=0}^{\infty}Pr\{N=n\}Pr\{\sum_{i=n}^{N}x_i\leq s|N=n\} \\
            &= \sum_{n=0}^{\infty}Pr\{N=n\}F_X^{*n}(s).
\end{aligned}$$

En donde $F_X^{*n}(s)$ es la convolución de las variables aleatorias $\{x_i\}_{i=1\dots N}$, o bien, la funcion de distribucion de probabilidad acumulada de $\sum_{i=n}^{N}x_i$ con $x_i \sim exp(\lambda_{exp}=1/`r  denom`)$. 
Sean $M_{\sum_{i=n}^{N}x_i}(t)$ y $M_{x_1}(t)$ las funciones generadoras de momentos de las variables aleatorias $\sum_{i=n}^{N}x_i$ y $x_1$ respectivamente, entonces, sin pÃ©rdida de generalidad, se tiene que

$$M_{\sum_{i=n}^{N}x_i}(t)=\prod_{i=1}^n M_{x_1}(t)=\prod_{i=1}^n\frac{\lambda_{exp}}{\lambda_{exp}+t}=\left(\frac{\lambda_{exp}}{\lambda_{exp}+t}\right)^n=\left(1-\frac{t}{\lambda_{exp}}\right)^{-n}.$$

Lo cual corresponde a la funcion generadora de momentos de una variable aleatoria distribuida Gamma con parametros de $\alpha = n$ y $\beta = \lambda_{exp}$. Ademas, la distribucion de la frecuencia es Poisson con parametros $\lambda_{Po}=`r lambda_po`$, entonces se sigue que
$$F_S(s)=\sum_{n=0}^{\infty}\frac{\lambda_{Po}^n e^{-\lambda_{Po}}}{n!}\int_0^s \frac{\lambda_{exp}^n}{\Gamma (n)}t^{n-1}e^{-\lambda_{exp}t} \, \mathrm{d}t $$
```{r plot S CDF, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# CDF Compound Poisson Distribution 
cdf.comp.pois <- function(x, lambda_po=4,lambda_exp=2) {
  # Dont take into account terms of the sum that are smaller than the following tolerance 
  tolerance <- 0.000001
  
  # Define the region of integration
  B <- c(0, x)
  
  # Calculate the sum
  # prob starts in zero because the severity is a positive random variable
  # First term
  prob <- 0 
  if (B[1] == 0) prob <- exp(-lambda_po)
  # The rest of terms
  n <- 1
  repeat {
    next_term <- exp(-lambda_po+n*log(lambda_po)-lfactorial(n)) * pgamma(B[2], shape=n, rate=lambda_exp)
    if (next_term < tolerance) {
      break
    }
    prob <- prob + next_term
    n <- n + 1
  }
  prob
}

# Get values to be ploted
getValues.cdf.comp.pois <- function(x, lambda_po=4, lambda_exp=2) sapply(x, FUN=cdf.comp.pois, lambda_po=lambda_po,lambda_exp=lambda_exp)

# Create a plot
p <- ggplot()

# Plot axes to make it look nicer
xAxis <- geom_hline(yintercept = 0, size = .5)
yAxis <-geom_vline(xintercept = 0, size = .5)

# Plot axes labels
cdf.comp.pois.labels <- labs(x = TeX('$s$'), y = TeX('$F_S(s)$'))
# Title
cdf.comp.pois.title <- ggtitle("Función de probabilidad acumulada \n de la severidad total")
# Center title
title.theme <- theme(plot.title = element_text(hjust = 0.5))

# Insert a horizontal line on y=1 because we are ploting a CDF
asinth.axis <- geom_hline(yintercept=1, size=.1)

# Color
if (colorImp) cdf.comp.pois.color=moradito else cdf.comp.pois.color="black"

# Create layer
cdf.comp.pois.layer <- stat_function(
  aes(x=c(0,10)),
  fun=getValues.cdf.comp.pois,
  args=list(lambda_po=lambda_po, lambda_exp=lambda_exp),
  color=cdf.comp.pois.color
)

# Adjust scales on axes
cdf.xlim <- xlim(0,80000)
cdf.ylim <- ylim(0.88,1)

# Plot the standard CDF
p + xAxis + yAxis + cdf.comp.pois.labels + cdf.comp.pois.title + title.theme + asinth.axis + cdf.comp.pois.layer + cdf.xlim + cdf.ylim
```

La funcion de densidad de probabilidad de la severidad total esta dada por
$$\begin{aligned}
  f_S(s) &= \frac{d}{ds}F_S(s)  \\
            &= \frac{d}{ds}\sum_{n=0}^{\infty}\frac{\lambda_{Po}^n e^{-\lambda_{Po}}}{n!}\int_0^s \frac{\lambda_{exp}^n}{\Gamma (n)}t^{n-1}e^{-\lambda_{exp}t} \, \mathrm{d}t \\
            &= \sum_{n=0}^{\infty}\frac{\lambda_{Po}^n e^{-\lambda_{Po}}}{n!}\frac{d}{ds}\int_0^s \frac{\lambda_{exp}^n}{\Gamma (n)}t^{n-1}e^{-\lambda_{exp}t} \, \mathrm{d}t \\
            &= \sum_{n=0}^{\infty}\frac{\lambda_{Po}^n e^{-\lambda_{Po}}}{n!} \frac{\lambda_{exp}^n}{\Gamma (n)}s^{n-1}e^{-\lambda_{exp}s}
\end{aligned}$$

```{r plot S PDF, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# PDF Compound Poisson Distribution 
pdf.comp.pois <- function(x, lambda_po=4,lambda_exp=2) {
  # Dont take into account terms of the sum that are smaller than the following tolerance 
  tolerance <- 0.0000000001
  prob <- 0
  # The rest of terms
  n <- 1
  repeat {
    next_term <- exp(-lambda_po+n*log(lambda_po)-lfactorial(n)) * dgamma(x, shape=n, rate=lambda_exp)
    if (next_term < tolerance) {
      break
    }
    prob <- prob + next_term
    n <- n + 1
  }
  prob
}

# Get values to be ploted
getValues.pdf.comp.pois <- function(x, lambda_po=4, lambda_exp=2) sapply(x, FUN=pdf.comp.pois, lambda_po=lambda_po,lambda_exp=lambda_exp)

# Plot axes labels
pdf.comp.pois.labels <- labs(x = TeX('$s$'), y = TeX('$f_S(s)$'))

# Title
pdf.comp.pois.title <- ggtitle("Función de densidad de probabilidad \n de la severidad total")

# Color
if (colorImp) pdf.comp.pois.color <- azulito else pdf.comp.pois.color <- "black"

# Create layer
pdf.comp.pois.layer <- stat_function(
  aes(x=c(0,10)),
  fun=getValues.pdf.comp.pois,
  args=list(lambda_po=lambda_po, lambda_exp=lambda_exp),
  color=pdf.comp.pois.color
)

# Adjust scales on axes
pdf.xlim <- xlim(0,80000)
pdf.ylim <- ylim(0,.00001)

# Plot the standard PDF
p + xAxis + yAxis + pdf.comp.pois.labels + pdf.comp.pois.title + title.theme + pdf.comp.pois.layer + pdf.xlim + pdf.ylim
```

A continuación se presenta una tabla que indica la probabilidad que se acumula en ciertos puntos de la distribución de la severidad total:
```{r percentile table, echo=FALSE, warning=FALSE}
# Percentiles vector
percentiles <- c(0,1,50,100,1000,10000,20000,40000,50000)
probs <- sapply(percentiles, FUN=cdf.comp.pois, lambda_po=lambda_po, lambda_exp=lambda_exp)
percentiles.df <- data.frame(percentiles, probs)
names(percentiles.df) <- c("s","F(s)")
kable(percentiles.df)
```

Como se puede observar, casi toda la probabilidad se acumula en valores muy pequeños para la severidad total.

\subsection{1.2 Simulación}

Para hacer simulaciones de la severidad total, primero hay que simular el número $N$ de siniestros ocurridos, el cual proviene de la distribución de Poisson con media $\lambda_{Po}=`r lambda_po`$. Una vez simulado el número $N$ de siniestros, se simulan $N$ siniestros, que son variables aleatorias exponenciales con parámetros $\lambda_{exp}=1/`r denom`$. Después de obtener los $N$ siniestros, se suman y de esta manera se obtiene un primer valor de la severidad total. Para concluir con la simulación, se repite el proceso anterior $10000$ veces. A continuación se muestran los primeros $100$ valores obtenidos de la simulacion:

```{r simulated values, echo=FALSE, warning=FALSE, message=FALSE}
severities <- c()
n <- 10000
for(i in 1:n) {
  N <- rpois(1,lambda=lambda_po)
  if (N != 0) {
    exps <- rexp(N, rate=lambda_exp)
    severities <- c(severities, sum(exps))
  } else {
    severities <- c(severities, 0)
  }
}
# Imprimiendo solo 100
kable(matrix(severities[1:100],9))

```

El histograma debe parecerse a la función de densidad de probabilidad (la linea punteada indica la seveidad promedio):

```{r histogram, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
if (colorImp) {
  hist.color <- moradito
  hist.fill <- moradito
  hist.alpha <- "0.2"
} else {
  hist.color <- "black"
  hist.fill <- "white"
  hist.alpha <- "1"
}

severities.df <- data.frame(severities)
breaks <- seq(1, 80000, by=1000)
hist.p <- ggplot(data=severities.df, aes(x=severities)) +
  geom_histogram(breaks=breaks, color=hist.color, fill=hist.fill, alpha=hist.alpha) + 
  labs(x = TeX('Severidad total$'), y = TeX('Frecuencia')) +
  ggtitle("Histograma") + title.theme
hist.p
```

En la siguiente gráfica se puede apreciar que, en efecto, el histograma se parece a la función de densidad de probabilidad (nótese el cambio de escala en el eje y):

```{r histogram with density, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# Plot the PDF with scaled histogram
# Create a plot
hist.den.p <- ggplot()
hist.p2 <- hist.den.p + xAxis + yAxis + pdf.comp.pois.labels + pdf.comp.pois.title + title.theme + pdf.xlim + pdf.ylim + geom_histogram(data=severities.df, aes(x=severities, y=.0000001*..count..), breaks=breaks, color=hist.color, fill=hist.fill, alpha=hist.alpha) + pdf.comp.pois.layer
hist.p2
# For more info about ploting density and histogram
# https://stat.ethz.ch/pipermail/r-help/2011-June/280588.html
```

\subsection{1.3 Comparación de los valores reales con los estimados}

Se vio que la severidad total, que sigue uns distribucion de Poisson Compuesta, está dada por
$$  S=\sum_{i=1}^{N}x_i.$$
Entonces el valor esperado de la severidad total está dado por
$$\begin{aligned}
  E(S) &= E\left(\sum_{i=1}^{N}x_i\right)=E(E(S|N=n))=E\left(\sum_{i=1}^{n}x_i\bigg\rvert N=n\right)=E(NE(x_1))  \\
            &= E\left(N\left(\frac{1}{\lambda_{exp}}\right)\right)=\frac{1}{\lambda_{exp}E(N)}=\frac{\lambda_{exp}}{\lambda_{Po}}=\frac{10000}{0.1}=1000
\end{aligned}$$
y su varianza está dada por
$$\begin{aligned}
  Var(S) &= Var(E(S|N)) + E(Var(S|N)) = Var(NE(x_1))+E(NVar(x_1)) \\
            &=\frac{1}{\lambda_{exp}}Var(N) + \frac{1}{\lambda_{exp}^2}E(N) = \frac{\lambda_{Po}}{\lambda_{exp}} + \frac{\lambda_{Po}}{\lambda_{exp}^2}=(10000)(0.1) + (10000)^2(0.1)=10001000
\end{aligned},$$
de manera que su desviacion estandar es la siguinete:  
$$\sqrt{Var(S)}=\sqrt{10001000}=3162.43577.$$
La función generadora de momentos de $S$ está dada por
$$M_{S}(t)=E\left(e^{N\ln\left(e^{x_{1}t}\right)}\right)=M_{N}(\ln\left(x_{1}t\right))=e^{\lambda_{Po}\left(\frac{\lambda_{exp}}{\lambda_{exp}+t}-1\right)} $$

se sigue que el sesgo de la distribucion es positivo y está dado por
$$E\left((S-E(S))^3\right)=\lambda_{Po}E(x_1^3)=\lambda_{Po}M^{(3)}_{x_1}(0)=\lambda_{Po}\frac{6}{\lambda_{exp}^3}=(0.1)(6)(10000)^3=6\times 10^{11},$$
y el coeficiente de Kurtosis es
$$ \frac{E(S^4)}{Var(S)^2}=\frac{M_{s}^{(4)}(0)}{\left(\frac{\lambda_{Po}}{\lambda_{exp}} + \frac{\lambda_{Po}}{\lambda_{exp}^2}\right)^2}=\frac{\frac{\lambda_{Po}^4+12\lambda_{Po}^3+36\lambda_{Po}^2+24\lambda_{Po}}{\lambda_{exp}^4}}{10001000^2}=\frac{\frac{(0.1)^4+12(0.1)^3+36(0.1)^2+24(0.1)}{(1/10000)^4}}{10001000^2}=277.1545. $$

Con valores simulados, se tienen las siguientes estimaciones para la severidad total

```{r comparando con valores estimados, echo=FALSE}
#value <- c("Media","Varianza","Sesgo","Kurtosis")
#formula <- c("$\\bar{s}=\\frac{1}{n}\\sum_{i=0}^{n}s_i$","$\\frac{1}{n-1}\\sum_{i=0}^{n}\\left(s_i-\\bar{s}\\right)$","${\\frac{{\\tfrac {1}{n}}\\sum _{i=1}^{n}(x_{i}-{\\overline{x}})^{3}}{\\left({\\tfrac {1}{n-1}}\\sum _{i=1}^{n}(x_{i}-{\\overline{x}})^{2}\\right)^{3/2}}}$","${\\frac{{\\tfrac{1}{n}}\\sum _{i=1}^{n}(s_{i}-{\\overline {s}})^{4}}{\\left({\\tfrac{1}{n}}\\sum _{i=1}^{n}(s_{i}-{\\overline {s}})^{2}\\right)^{2}}}-3$")
#real <- c(1000,10001000,"$6\\times10^{11}$",277.1545)
#estimate <- c(mean(severities),var(severities),skew(severities),kurtosis(severities))
#df.values <- data.frame(value, formula, estimate, real)
#names(df.values) <- c("Valor","Formula","Estimado","Real")
#kable(df.values)
```
Estimaciones por Jack Knife
```{r, echo=FALSE}
##Media
JKMean<-NULL
for(i in 1:10000){
  JKMean[i]= (sum(severities)-severities[i])/(10000-1)
}
JKMean1=mean(JKMean)
#confint(JKMean,JKMean,level=.95)

##Varianza
JKVar<-NULL
for(i in 1:10000){
  JKVar[i]= (var(severities[-i]))
}
JKVar1=mean(JKVar)
sqrt(JKVar1)
var(severities)


##Sesgo
JKBias<-NULL
for(i in 1:10000){
  JKBias[i]= mean((severities[-i]-mean(severities))^3)
}
JKBias1=mean(JKBias)


```


\subsection{1.4 Aproximación de la distribución de la severidad total}

La función de distribución Poisson Compuesta puede ser aproximada mediante una distribución Gamma trasladada con los siguientes parámetros: 
$$\alpha=\frac{4\lambda_{Po}E\left(x_1^{2}\right)^3}{E\left(x_1^3\right)^2}=\frac{4\lambda_{Po}\left(\frac{2}{\lambda_{exp}^2}\right)^3}{\left(\frac{6}{\lambda_{exp}^3}\right)^2}=\frac{4}{45}, \quad \beta=\frac{2E\left(x_1^2\right)}{E\left(x_1^3\right)}=\frac{2\left(\frac{2}{\lambda_{exp}^2}\right)}{\left(\frac{6}{\lambda_{exp}^3}\right)}=\frac{1}{15000}$$

y un desplazamiento
$$x_0=\lambda_{Po}E\left(x_1\right)-\frac{2\lambda_{Po}E(x_1^2)^2}{E(x_1^3)}=\lambda_{Po}\left(\frac{1}{\lambda_{exp}}\right)-\frac{2\lambda_{Po}\left(\frac{2}{\lambda_{exp}^2}\right)^2}{\left(\frac{6}{\lambda_{exp}^3}\right)}=-\frac{1000}{3}$$
Observe que la aproximación es buena para valores distantes de la media de la distribución Poisson Compuesta: 
```{r densidad vs aproximacion gamma, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# PDF Gamma Trasladada
getValues.pdf.gamma.tras <- function(x, desp=1, alpha=4, beta=2) {
  dgamma(x-desp, shape = alpha, rate=beta)
}

# Stablish theme
comp.theme <- theme(
  #panel.grid.major = element_blank(), 
  #panel.grid.minor = element_blank(),
  #panel.background = element_blank(),
  
  # Legend position
  legend.position = c(1, 1),
  legend.justification = c("right", "top"),
  legend.box.just = "right",
  
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8)
)

# Title
comp.title <- ggtitle("Densidad de la severidad total \n vs. \n aproximacion Gamma")

# Number of different function to be ploted
types <- c("1","2")
# Create plot
comp.p <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
# Create layers
pdf.comp.pois.layer.path <- geom_path(
  data=data.frame(type="1"),
  stat="function",
  fun = getValues.pdf.comp.pois,
  args=list(lambda_po=lambda_po, lambda_exp=lambda_exp),
  aes(colour=type, linetype=type, size=type)
)
pdf.gam.layer <- stat_function(
  data=data.frame(type="2"),
  fun = getValues.pdf.gamma.tras,
  args = list(desp=-1000/3, alpha=4/45, beta=1/15000),
  aes(colour=type, linetype=type, size=type)
)
# Colors and line types
if (colorImp) {
  aprox.color <- moradito
  colors <- c(pdf.comp.pois.color,aprox.color)
  sizes <- c(.5,.5)
  linetypes <-c(1,1)
} else {
  colors <- c("black", "black")
  sizes <- c(.5,.5)
  linetypes <-c(1,5)
}
# Legends
comp.an <- labs(TeX('Severidad total'), TeX('aproximacion Gamma'))

# Plot
comp.p + comp.theme + xAxis + yAxis + pdf.comp.pois.labels + comp.title + pdf.xlim + pdf.ylim + pdf.comp.pois.layer.path + pdf.gam.layer +
  scale_colour_manual(
    #name="Densidades", 
    name=NULL, 
    values=colors,
    labels = comp.an,
    breaks = types
  ) + 
  scale_linetype_manual(
    #name="Densidades", 
    name=NULL,
    values = linetypes,
    labels = comp.an,
    breaks = types
  ) +
  scale_size_manual(
    #name="Densidades", 
    name=NULL,
    values = sizes,
    labels = comp.an,
    breaks = types
  )

```
Para valores cercanos a la media es mas conveniente aproximar la distribucion Poisson Compuesta con una distribucion Normal con los siguientes parametros: 
$$\mu=E(N)E(X_1)=\frac{\lambda_{Po}}{\lambda_{exp}}=(0.1)(10000)=1000, $$
$$\sigma^2={E(x_1)}Var(N) + Var(x_1)E(N) = \frac{\lambda_{Po}}{\lambda_{exp}} + \frac{\lambda_{Po}}{\lambda_{exp}^2}=(10000)(0.1) + (10000)^2(0.1)=10001000$$
Sin embargo, esto solo funciona para valores grandes de $\lambda_{Po}$. En el siguinete gráfico se muestra que la aproximación Normal para valores alrededor de la media es bastante mala, pues como $\lambda_{Po}$ es pequeña la media de la suma de variables aleatorias no corverge a la media de la aproximación Normal:

```{r densidad vs aproximacion normal, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
# PDF Compound Poisson Distribution 
dpoiscomp <- function(x, lambda_po=4,lambda_exp=2) {
  # Dont take into account terms of the sum that are smaller than the following tolerance 
  tolerance <- 0.0000000001
  prob <- 0
  # The rest of terms
  n <- 1
  repeat {
    next_term <- exp(-lambda_po+n*log(lambda_po)-lfactorial(n)) * dgamma(x, shape=n, rate=lambda_exp)
    if (next_term < tolerance) {
      break
    }
    prob <- prob + next_term
    n <- n + 1
  }
  prob
}

# Get values to be ploted
getValues.pdf.pois.comp <- function(x, lambda_po=4, lambda_exp=2) sapply(x, FUN=dpoiscomp, lambda_po=lambda_po,lambda_exp=lambda_exp)

# PDF Gamma Trasladada
getValues.pdf.gamma.tras <- function(x, desp=1, alpha=4, beta=2) {
  dgamma(x-desp, shape = alpha, rate=beta)
}

# Create a plot
p <- ggplot()

# Stablish background and leyend position
theme <- theme(
  # Background
  #panel.grid.major = element_blank(), 
  #panel.grid.minor = element_blank(),
  #panel.background = element_blank(),
  
  # Legend position
  legend.position = c(1, 1),
  legend.justification = c("right", "top"),
  legend.box.just = "right",
  
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8)
)
# Plot axes to make it look nicer
xAxis <- geom_hline(yintercept = 0, size = .1)
yAxis <-geom_vline(xintercept = 0, size = .1)

# Plot PDFs
# Labels
labels <- labs(x = TeX('$s$'), y = TeX('$f_S(s)$'))
# Title
title <- ggtitle("Densidad de la severidad total \n vs. \n aproximación Normal")
# Empty plot
# Number of different function to be ploted
types <- c("1","2")
# Create plot
p1 <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
# Create layers
pdf1 <- geom_path(
  data=data.frame(type="1"),
  stat="function",
  fun = getValues.pdf.pois.comp,
  args=list(lambda_po=0.1, lambda_exp=1/10000),
  aes(colour=type, linetype=type, size=type)
)
pdf2 <- stat_function(
  data=data.frame(type="2"),
  fun = dnorm,
  args = list(mean=1000, sd=sqrt((0.1)*(10000^2))),
  aes(colour=type, linetype=type, size=type)
)
# Colors and line types
if (colorImp) {
  aprox.color <- moradito
  colors <- c(pdf.comp.pois.color,aprox.color)
  sizes <- c(.5,.5)
  linetypes <-c(1,1)
} else {
  colors <- c("black", "black")
  sizes <- c(.5,.5)
  linetypes <-c(1,5)
}
# Legends
an <- labs(TeX('Severidad total'), TeX('aproximación Normal'))

# Plot
p1 + theme + xAxis + yAxis + labels + title + pdf.xlim + pdf.ylim + pdf1 + pdf2 +
  scale_colour_manual(
    #name="Densidades", 
    name=NULL, 
    values=colors,
    labels = an,
    breaks = types
  ) + 
  scale_linetype_manual(
    #name="Densidades", 
    name=NULL,
    values = linetypes,
    labels = an,
    breaks = types
  ) +
  scale_size_manual(
    #name="Densidades", 
    name=NULL,
    values = sizes,
    labels = an,
    breaks = types
  )
```

En este caso, la media de la distribución de la frecuencia de los siniestros en un año es un valor muy pequeño dado por $\lambda_{Po}=`r lambda_po`$, entonces, lo más recomendable para estimar percentiles alrededor de la media, es aproximar la función de distribución mediante una distribución lognormal:

```{r densidad vs aproximacion lognormal, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
get.dlnorm <- function(x, corr=0, meanlog=0, sdlog=1) {
  dlnorm(x-corr, meanlog = meanlog, sdlog = sdlog)
}

# Plot PDFs
# Labels
labels.comp <- labs(x = TeX('$s$'), y = TeX('$f_S(s)$'))
# Title
title.comp <- ggtitle("Densidad de la severidad total \n vs. \n aproximación Lognormal")
# Empty plot
# Number of different function to be ploted
types <- c("1","2")
# Create plot
p1 <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
# Create layers
pdf1.pois.comp <- geom_path(
  data=data.frame(type="1"),
  stat="function",
  fun = getValues.pdf.pois.comp,
  args=list(lambda_po=0.1, lambda_exp=1/10000),
  aes(colour=type, linetype=type, size=type)
)
pdf2.log.norm <- stat_function(
  data=data.frame(type="2"),
  fun = get.dlnorm,
  args = list(corr=-5000,meanlog = 4, sdlog = 5),
  aes(colour=type, linetype=type, size=type)
)
# Legends
an <- labs(TeX('Severidad total'), TeX('aproximación Lognormal'))
# Plot
p1 + theme + xAxis + yAxis + labels.comp + title.comp + 
  xlim(0,80000) + 
  ylim(0,.00001) + 
  pdf1.pois.comp + pdf2.log.norm +
  scale_colour_manual(
    #name="Densidades", 
    name=NULL, 
    values=colors,
    labels = an,
    breaks = types
  ) + 
  scale_linetype_manual(
    #name="Densidades", 
    name=NULL,
    values = linetypes,
    labels = an,
    breaks = types
  ) +
  scale_size_manual(
    #name="Densidades", 
    name=NULL,
    values = sizes,
    labels = an,
    breaks = types
  )
```

Esta proximación Lognormal tiene un corrimiento igual $5000$, una media de $4$ y una desviación estándar de $5$.


\subsection{1.5 distribucion empirica}

La distribución empírica de la severidad total está dada por

```{r, Preg. 2.3 Distribucion empirica de la severidad, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
df <- data.frame(severities)
theme <- theme(
  # Center plot title
  plot.title = element_text(hjust = 0.5)
)
if (colorImp) emp.color <- azulito else emp.color <- "black"
ggplot() + stat_ecdf(data=df, aes(x=severities), geom = "step", color=emp.color) +
  labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$')) + asinth.axis + xAxis + yAxis +
  ggtitle("distribución empírica de la severidad total") + theme + cdf.ylim
```

Entonces, un intervalo de confianza a un nivel de $(1-\alpha)100\%$ para distribución de la serveridad total $F_{S}(s)$ está dado por
$$\left(F_{s_n}(s)-\sqrt\frac{\ln\frac{2}{\alpha}}{2n}, F_{s_n}(s)+\sqrt\frac{\ln\frac{2}{\alpha}}{2n}\right)$$
Si se construyen los intervalos de confianza al $95\%$ para $F_S(s)$ utilizando la distribución empírica se tiene lo siguiente:
```{r, echo=FALSE, warning=FALSE}
Fn <- c(1:10000)
ordenados <- sort(severities)
alfa <- 0.05
limsup <- c(1:10000)
liminf <- c(1:10000)
for (i in 1:10000) {
  cuenta<-0
  for (j in 1:10000) {
    if (ordenados[j]<=ordenados[i]){
      cuenta=cuenta+1
    }
  }
  Fn[i] <- cuenta/10000
  limsup[i] <- Fn[i] + sqrt(log(2/alfa)/(2*10000))
  if (limsup[i] > 1) limsup[i] <- 1
  liminf[i] <- Fn[i] - sqrt(log(2/alfa)/(2*10000))
}
```

```{r, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
types <- c("1","2","3","4")
p <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
theme <- theme(
  # Leyend position
  legend.position = c(1, 0),
  legend.justification = c("right", "bottom"),
  legend.box.just = "right",
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8)
)
xAxis <- geom_hline(yintercept = 0, size = .1)
yAxis <-geom_vline(xintercept = 0, size = .1)
labels <- labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$'))
title <- ggtitle("Intervalos de confianza al 95% para \n la distribución de la severidad total")

fun2 <- geom_step(data=data.frame(ordenados,limsup, type="1"), aes(x=ordenados, y=limsup, colour=type, linetype=type, size=type)) 
fun1 <- geom_step(data=data.frame(ordenados,Fn, type="3"), aes(x=ordenados, y=Fn, colour=type, linetype=type, size=type)) 
fun3 <- geom_step(data=data.frame(ordenados,liminf, type="4"), aes(x=ordenados, y=liminf, colour=type, linetype=type, size=type)) 
cdf.layer <- geom_path(
  data=data.frame(type="2"),
  stat="function",
  fun=getValues.cdf.comp.pois,
  args=list(lambda_po=lambda_po, lambda_exp=lambda_exp),
  aes(colour=type, linetype=type, size=type)
)
an <- labs(TeX('Cota superior'), TeX('teórica'),TeX('empírica'), TeX('Cota inferior'))

if (colorImp) {
  colors <- c("#800080", "#ffa500", "#add8e6","purple")
  sizes <- c(.4,2,1,.4)
  linetypes <- c(1,1,1,1)
} else {
  colors <- c("black","black","black","black")
  sizes <- c(1,1,1,1)
  linetypes <-c(1,2,3,4)
}

p+theme+xAxis+yAxis+labels+title+cdf.layer+fun1+fun2+fun3+cdf.xlim+cdf.ylim+ asinth.axis +
scale_colour_manual(
  #name="Name",
  name = NULL,
  values = colors,
  labels = an,
  breaks = types
) +
scale_linetype_manual(
  #name="Name",
  name = NULL,
  values = linetypes,
  labels = an,
  breaks = types
) +
scale_size_manual(
  #name="Name",
  name = NULL,
  values = sizes,
  labels = an,
  breaks = types
)

```

\section{Estimacion de la frecuencia y severidad}

\subsection{2.1 simulacion de los pares $\left(T_i,x_i\right)$}

Como la frecuencia de los eventos en un año se distribuye Poisson con media $\lambda_{Po}=`r lambda_po`$, entonces las diferencias de tiempo entre las ocurrencias de los eventos siguen una distribución exponencial con media $1/\lambda=\lambda_{Po}=`r lambda_po`$, es decir, los tiempos inter-arribos siguen una distribución exponencial con media $1/\lambda=\lambda_{Po}=`r lambda_po`$. Simulando los tiempos inter-arribos exponenciales, se puden simular las fechas de ocurrencia de los siniestros a partir de hoy. A continuación se muestran las primeras $5$ y las últimas $5$ realizaciones de las fechas de los siniestros y los montos correspondientes distribuidos exponencialmente con media $1/\lambda_{exp}=`r denom`$:

```{r simulated pairs, echo=FALSE, warning=FALSE, message=FALSE}
start  <- 2018
# Tiempo interarribos
per <- rexp(10000, rate = lambda_po)
# Obtener las fechas de los siniestros
per[1] <- per[1]+start
# Fechas de los siniestros
PerTot <- cumsum(per)
# Montos de los siniestros
montos <- rexp(10000 ,rate=lambda_exp)

convert.seconds <- function(x) {
  a <- x-2018
  b <- a*31556952
  c <- as.POSIXct(b, origin = "2018-01-01", tz = "UTC")
  as.character(c)
}
dates <- sapply(PerTot, convert.seconds)

archivo <- data.frame(dates, montos)
names(archivo) <- c("Fechas", "Montos")
primeras <- head(archivo, 5)
kable(primeras, row.names = NA)
```

$$\vdots  $$
```{r cola, echo=FALSE, warning=FALSE}
ultimas <- tail(archivo, 5)
chacada <- data.frame(ultimas$Fechas, ultimas$Montos)
names(chacada) <- c("Fechas", "Montos")
kable(chacada)
```

\pagebreak

\subsection{2.2 Serie de tiempo} 

```{r total de informacion, echo=FALSE, warning=FALSE}
conver <- function(x) x-2018
tiempoEnAnios <- sapply(PerTot, conver)
```

A continuación se muestra la serie de tiempo generada con la informacion de $`r floor(max(tiempoEnAnios))`$ años:

```{r, serie de tiempo, echo=FALSE, fig.height = 12, fig.width = 10}
# Ploting time series
# https://www.neonscience.org/dc-time-series-plot-ggplot-r
theme <- theme(
  # Leyend position
  legend.position = c(1, 0),
  legend.justification = c("right", "bottom"),
  legend.box.just = "right",
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8),
  axis.text.x=element_text(size=14)
)
fechonsias <- archivo[[1]]
funciononsia <- function(x) {
  value <- NA
  if (match(x,fechonsias) %% 500 !=0) {
    value <- NA
  } else {
    fechaSeparada <- strsplit(as.character(x), " ")
    value <- fechaSeparada[[1]][1]
  }
  value
}
fechunias <- sapply(fechonsias, funciononsia)
montunios <- archivo[[2]]
dfonsio <- data.frame(fechunias, montunios)
names(dfonsio) <- c("fechonson", "montonson")

ggplot(data = dfonsio, aes(x = c(1:10000), y = montonson, group = 1)) + 
  geom_bar(stat="identity", color = "#00AFBB", size = .1, alpha=.1, na.rm = TRUE) +
  geom_point(size=.005, color="blue", alpha=0.5) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5)) + 
  scale_x_discrete(limits=dfonsio[[1]], na.translate = FALSE) + xlab("Fecha") + ylab("Monto") + 
  geom_hline(yintercept=10000, size=1, color="cyan") + coord_flip() + scale_y_reverse() + theme
```

La linea azul claro indica la media de la distribución teorica a partir de la cual fueron simulados los datos.

\subsection{2.3 Distribución empírica de la severidad}

Con los valores simulados de los montos, se puede construir la función de distribución de probabilidad empírica de la severidad:

```{r calculo empirica severidad, echo=FALSE, warning=FALSE}
Fn.sev <- c(1:10000)
empirica<-ecdf(montos)
montosOrdenados <- sort(montos)
alfa.sev <- 0.05
limsup.sev <- c(1:10000)
liminf.sev <- c(1:10000)
for (i in 1:10000) {
  cuenta<-0
  for (j in 1:10000) {
    if (montosOrdenados[j]<=montosOrdenados[i]){
      cuenta=cuenta+1
    }
  }
  Fn.sev[i] <- cuenta/10000
  limsup.sev[i] <- Fn.sev[i] + sqrt(log(2/alfa.sev)/(2*10000))
  if (limsup.sev[i] > 1) limsup.sev[i] <- 1
  liminf.sev[i] <- Fn.sev[i] - sqrt(log(2/alfa)/(2*10000))
}
```

```{r, distribucion empirica de la severidad, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
types <- c("1","2","3","4")
p <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
theme <- theme(
  # Leyend position
  legend.position = c(1, 0),
  legend.justification = c("right", "bottom"),
  legend.box.just = "right",
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8)
)
xAxis <- geom_hline(yintercept = 0, size = .1)
yAxis <-geom_vline(xintercept = 0, size = .1)
labels <- labs(x = TeX('$m$ (montos)'), y = TeX('$F_{M_n}(m)$'))
title <- ggtitle("Intervalos de confianza al 95% para \n la distribución de la severidad")

montosOrdenados <- sort(montos)
fun2 <- geom_step(data=data.frame(montosOrdenados,limsup, type="1"), aes(x=montosOrdenados, y=limsup.sev, colour=type, linetype=type, size=type)) 
fun1 <- geom_step(data=data.frame(montosOrdenados,Fn, type="3"), aes(x=montosOrdenados, y=Fn.sev, colour=type, linetype=type, size=type)) 
fun3 <- geom_step(data=data.frame(montosOrdenados,liminf, type="4"), aes(x=montosOrdenados, y=liminf.sev, colour=type, linetype=type, size=type)) 
cdf.layer <- geom_path(
  data=data.frame(type="2"),
  stat="function",
  fun=pexp,
  args=list(rate=lambda_exp),
  aes(colour=type, linetype=type, size=type)
)
an <- labs(TeX('Cota superior'), TeX('teórica'),TeX('empírica'), TeX('Cota inferior'))

if (colorImp) {
  colors <- c("#800080", "#ffa500", "#add8e6","purple")
  sizes <- c(.4,2,1,.4)
  linetypes <- c(1,1,1,1)
} else {
  colors <- c("black","black","black","black")
  sizes <- c(1,1,1,1)
  linetypes <-c(1,2,3,4)
}

p+theme+xAxis+yAxis+labels+title+cdf.layer+fun1+fun2+fun3+cdf.xlim+ylim(0,1)+ asinth.axis +
scale_colour_manual(
  #name="Name",
  name = NULL,
  values = colors,
  labels = an,
  breaks = types
) +
scale_linetype_manual(
  #name="Name",
  name = NULL,
  values = linetypes,
  labels = an,
  breaks = types
) +
scale_size_manual(
  #name="Name",
  name = NULL,
  values = sizes,
  labels = an,
  breaks = types
)
```

Dado el numero de observaciones (simulaciones) $n=10000$, se tiene que la función de distribución empírica de la severidad se aproxima bastante bien a la función de distribución teórica y la longitud de los intervalos de confianza no es muy grande. Note que las cotas superiores e inferiores "abrazan estrechamente" a la función de distribución teórica. 

\subsection{2.4 Histograma de la severidad}

Agrupando los montos de los siniestros en intervalos de longitud $5000$, se obtiene el siguinete histograma:
```{r histograma severidad, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
if (colorImp) {
  hist.color <- moradito
  hist.fill <- moradito
  hist.alpha <- "0.2"
} else {
  hist.color <- "black"
  hist.fill <- "white"
  hist.alpha <- "1"
}

montos.df <- data.frame(montos)
breaks <- seq(1, 80000, by=5000)
hist.p <- ggplot(data=montos.df, aes(x=montos)) +
  geom_histogram(breaks=breaks, color=hist.color, fill=hist.fill, alpha=hist.alpha) + 
  labs(x = TeX('Severidad$'), y = TeX('Frecuencia')) +
  ggtitle("Histograma de la severidad") + title.theme
hist.p
```

Con un escalamiento adecuado sobre las frecuencias (note el cambio en la escala en el eje $y$), se puede observar la enorme similitud entre la densidad y el histograma:

```{r  histograma con densidad sev, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}

pdf.sev.layer <- stat_function(
  aes(x=c(0,10)),
  fun=dexp,
  args=list(rate=lambda_exp),
  color=azulito
)

hist.p2.sev <- ggplot() + xAxis + yAxis + title.theme + pdf.xlim + ylim(0,.0001) + geom_histogram(data=montos.df, aes(x=montos, y=.00000002*..count..), breaks=breaks, color=hist.color, fill=hist.fill, alpha=hist.alpha) + pdf.sev.layer +
  ggtitle("función de densidad de probabilidad \n de la severidad total") +
  labs(x = TeX('$s$'), y = TeX('$f_S(s)$'))
hist.p2.sev
```

\subsection{2.5 Características de la distribución empírica}

Sea $M$ la variable aleatoria que indica el monto de un siniestro, entonces el valor esperado de $M$ limitado a $l$ está dada por

$$  E\left( \min{M, l} \right)=\int_{0}^{l}S_{x_1}(t) \, \mathrm{d}t=\int_{0}^{l} e^{-\lambda_{exp}t} \, \mathrm{d}t = \frac{1-e^{-\lambda_{exp}l}}{\lambda_{exp}} $$

A continuacion se presenta a la \textbf{media limitada} de los montos como funcion del limite:

```{r media limitada, echo=FALSE, warning=FALSE, message=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
sortedSeverities <- sort(unique(montos))
 
# Medias limitadas usando datos empiricos
n=10000
mediasLimitadas <- NULL
for (i in 1:length(sortedSeverities)) {
  u <- sortedSeverities[i]
  mediasLimitadas[i] <- (sum(montos>u)*u + sum(montos[montos<=u]))/n
}

# Media limitada analiticamente como funcion del limite
limitedMean <- function(x) (1-exp(-lambda_exp*x))/(lambda_exp)

types <- c("1","2")
p.m <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
theme <- theme(
   # Leyend position
   legend.position = c(1, 0),
   legend.justification = c("right", "bottom"),
   legend.box.just = "right",
   # Legend boxes
   legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
   legend.key.width = unit(1.5, "lines"),
   legend.background = element_rect(fill="transparent"),
   # Center plot title
   plot.title = element_text(hjust = 0.5),
   legend.text=element_text(size=8)
 )
 xAxis <- geom_hline(yintercept = 0, size = .1)
 yAxis <-geom_vline(xintercept = 0, size = .1)
 labels <- labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$'))
 title <- ggtitle("Función media limiatada")
 
 points <- geom_point(
   data=data.frame(sortedSeverities, mediasLimitadas, type="1"),
   aes(x=sortedSeverities, y=mediasLimitadas, colour=type, linetype=type, size=type),
   shape=0
 )
 
 curve <- geom_path(
   data=data.frame(type="2"),
   stat="function",
   fun=limitedMean,
   aes(colour=type, linetype=type, size=type)
 )
 
 an <- labs(TeX('Valores simulados'), TeX('Media limitada'))
 
 if (colorImp) {
   colors <- c(moradito,azulito)
   sizes <- c(.3,1)
   linetypes <- c(1,1)
 } else {
   colors <- c("black","black")
   sizes <- c(1,1)
   linetypes <-c(1,2)
 }
 
 p.m+theme+xAxis+yAxis+labs(x="Limite", y="Media limitada")+title+curve+points+xlim(0,80000)+ylim(0,10000)+ asinth.axis +
 scale_colour_manual(
   #name="Name",
   name = NULL,
   values = colors,
   labels = an,
   breaks = types
 ) +
 scale_linetype_manual(
   #name="Name",
   name = NULL,
   values = linetypes,
   labels = an,
   breaks = types
 ) +
 scale_size_manual(
   #name="Name",
   name = NULL,
   values = sizes,
   labels = an,
   breaks = types
 )
```

Como los montos son distribuidos exponencialmente con media $E(x_1)=1/\lambda_{exp}$, no se observan indicios de una distribucion de cola pesada.
 
 La \textbf{media en exceso} como funcion del umbral es la siguiente:

```{r media en exceso, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
 # Medias en exceso usando datos empiricos
 mediasEnExceso <- NULL
 for(i in 1:length(sortedSeverities)){
   d <- sortedSeverities[i]
   mediasEnExceso[i] <- sum(montos[montos>d]-d)/sum(montos>d)
 }
 
# Media limitada analiticamente como funcion del limite
 excesMean <- function(x, param=1) param
 
 types <- c("1","2")
 p.m <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
 theme <- theme(
   # Leyend position
   legend.position = c(1, 0),
   legend.justification = c("right", "bottom"),
   legend.box.just = "right",
   # Legend boxes
   legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
   legend.key.width = unit(1.5, "lines"),
   legend.background = element_rect(fill="transparent"),
   # Center plot title
   plot.title = element_text(hjust = 0.5),
   legend.text=element_text(size=8)
 )
 xAxis <- geom_hline(yintercept = 0, size = .1)
 yAxis <-geom_vline(xintercept = 0, size = .1)
 labels <- labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$'))
 title <- ggtitle("Función media en exceso")
 
 points <- geom_point(
   data=data.frame(sortedSeverities, mediasEnExceso, type="1"),
   aes(x=sortedSeverities, y=mediasEnExceso, colour=type, linetype=type, size=type),
   shape=1
 )
 
 curve <- geom_path(
   data=data.frame(type="2"),
   stat="function",
   fun=excesMean,
   args=list(param=10000),
   aes(colour=type, linetype=type, size=type)
 )
 
 an <- labs(TeX('Valores simulados'), TeX('Media en exceso'))
 
 p.m+theme+xAxis+yAxis+labs(x="Umbral", y="Media en exceso")+title+curve+points+xlim(0,80000)+ylim(0,15000)+ asinth.axis +
 scale_colour_manual(
   #name="Name",
   name = NULL,
   values = colors,
   labels = an,
   breaks = types
 ) +
 scale_linetype_manual(
   #name="Name",
   name = NULL,
   values = linetypes,
   labels = an,
   breaks = types
 ) +
 scale_size_manual(
   #name="Name",
   name = NULL,
   values = sizes,
   labels = an,
   breaks = types
 )
```

Claramente los montos de los siniestros no son un fenomeno de colas pesadas, pues se observa que la media en exceso descrece.
 
La \text{mortalidad empirica} como funcion del monto es la siguiente:

```{r mortalidad empirica, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
 # Mortalidad utilizando datos empirica
 fn <- NULL
 mortalidades <- NULL
 for (i in 1:length(sortedSeverities)) {
   fn[i] <- empirica(sortedSeverities[i+1])-empirica(sortedSeverities[i])
   mortalidades[i] <- fn[i]/(1-empirica(sortedSeverities[i]))
 }
 
 # Funcion de mortalidad
 hazardRate <- function(x, param=1) 1/param
 
 types <- c("1","2")
 p.m <- ggplot(data.frame(type=types), aes(colour=type, linetype=type, size=type))
 theme <- theme(
   # Leyend position
   legend.position = c(0.05, 1),
   legend.justification = c("left", "top"),
   legend.box.just = "right",
   # Legend boxes
   legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
   legend.key.width = unit(1.5, "lines"),
   legend.background = element_rect(fill="transparent"),
   # Center plot title
   plot.title = element_text(hjust = 0.5),
   legend.text=element_text(size=8)
 )
 xAxis <- geom_hline(yintercept = 0, size = .1)
 yAxis <-geom_vline(xintercept = 0, size = .1)
 labels <- labs(x = TeX('$s$'), y = TeX('$F_{S_n}(s)$'))
 title <- ggtitle("Función mortalidad")
 
 points <- geom_point(
   data=data.frame(sortedSeverities, mortalidades, type="1"),
   aes(x=sortedSeverities, y=mortalidades, colour=type, linetype=type, size=type),
   shape=1
 )
 
 curve <- geom_path(
   data=data.frame(type="2"),
   stat="function",
   fun=hazardRate,
   args=list(param=10000),
   aes(colour=type, linetype=type, size=type)
 )
 
 an <- labs(TeX('Valores simulados'), TeX('Mortalidad = 1/10000'))
 
 p.m+theme+xAxis+yAxis+labs(x="Montos", y="Mortalidad")+title+curve+points+xlim(0,85000)+ylim(0,1)+ asinth.axis +
 scale_colour_manual(
   #name="Name",
   name = NULL,
   values = colors,
   labels = an,
   breaks = types
 ) +
 scale_linetype_manual(
   #name="Name",
   name = NULL,
   values = linetypes,
   labels = an,
   breaks = types
 ) +
 scale_size_manual(
   #name="Name",
   name = NULL,
   values = sizes,
   labels = an,
   breaks = types
 )
```

\subsection{2.6 Prueba Ji-Cuadrada de Pearson}

Se agrupan los datos de tal forma que todas las frecuencias de los datos observados sean mayores a 5:

```{r Prueba Chi-Square, echo=FALSE, warning=FALSE}
##Datos agrupados
nj<-NULL
agrupados1<-data.frame(cj_1=seq(0,max(sortedSeverities),5000), cj=seq(5000,max(sortedSeverities)+5000,5000))
                      
for(i in 1:length(agrupados1$cj_1)){
  nj[i]<-sum(montos<agrupados1$cj[i] & agrupados1$cj_1[i] <= montos)
}
agrupados1[3]<-nj
row.names(agrupados1)<-c(1:length(agrupados1$V3))

 agrupados<-agrupados1
k=1
while(agrupados$V3[k]>=5){
  aux <- k
k=k+1
}

  aux2<-0
  for(i in 1:(length(agrupados$V3)-aux)){
    aux2<-aux2+agrupados$V3[aux+i]
  }
agrupados$V3[aux+1]<-aux2

for(i in 1:(length(agrupados$cj)-aux-1)){
   agrupados$V3[aux+1+i]<-0
}

agrupados$cj[aux+1]<- agrupados$cj[length(agrupados$cj_1)]
 agrupados<-subset(agrupados, agrupados$V3>0) 
 agrupados<-rbind(agrupados,1)
 agrupados$cj_1[length(agrupados$V3)]<-agrupados$cj[length(agrupados$V3)-1]
 agrupados$cj[length(agrupados$cj)]<-"inf"
 agrupados$V3[length(agrupados$V3)]<-0
 kable(agrupados)

 ##Estimo el parámetro para una exponencial por MV
theta<-1/mean(montos)
E<-NULL
E[1]<-10000*(pexp(5000,theta))
for(i in 2:(length(agrupados$cj_1)-1)){
  E[i]<-10000*(pexp(as.numeric(agrupados$cj[i]),theta)-pexp(as.numeric(agrupados$cj_1[i]),theta))
}
E[length(agrupados$cj_1)]=10000*(1-pexp(as.numeric(agrupados$cj_1[length(agrupados$cj_1)]),theta))
agrupados[4]<-E

Q<-NULL
for(i in 1:(length(agrupados$cj_1))) {
  Q[i]<-((as.numeric(agrupados$V3[i])-as.numeric(agrupados$V4[i]))^2)/as.numeric(agrupados$V3[i])
}
Q[length(agrupados$cj_1)]=0
agrupados[5]<-Q
sumaQ<-sum(Q)
gl<-length(agrupados$cj_1)-3
test<-qchisq(.95,gl)

Rechazar <- sumaQ > test
```

Realizando la prueba Ji-Cuadrada, se tiene que la estadistica de prueba toma el siguite valor:
$$Q=`r sumaQ` < \chi_{(`r gl`),0.05}^2 = `r test`$$


Por lo tanto, la hipótesis de que la severidad sigue una distribución exponencial con media $10000$ no es rechazada.

```{r prueba, echo=FALSE, warning=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
ggplot(data=data.frame(num=c("1","2","3"))) +
  stat_function(
    fun = dchisq,
    args = list(df = gl),
    xlim = c(test, 40),
    geom = "area",
    fill = moradito,
    color = "transparent",
    alpha = ".5"
    ) +
stat_function(
  fun = dchisq,
  args = list(df = gl),
  color = moradito
) +
geom_vline(
  aes(
    xintercept = sumaQ
  ),
  color = azulito
) +
geom_vline(
  aes(
    xintercept = test
  ),
  color="black"
) +
xlim(0,40) +
ylim(0,0.1) +
  labs(x = TeX('$x$'), y = TeX('$ f_{\\chi_{(14)}^2}(x) $')) + xAxis + yAxis + ggtitle("Prueba Ji-Cuadrada al 95%") + theme +
  annotate("text", x = 30, y = 0.01, label = "alpha==0.05", parse=TRUE) + 
  annotate("text", x = 10, y = 0.005, label = paste("Q==", sumaQ), parse=TRUE) +
  annotate("text", x = 30, y = 0.075, label = paste("{chi^{2}} ==", test), parse=TRUE)  
```

\subsection{2.7 Q-Q Plot}

Derivado del q-q plot podemos concluir que el modelo subestima las probabilidades de eventos de monto alto:

```{r q-q plot, echo=FALSE, warning=FALSE, message=FALSE, fig.width=4.5, fig.height=3, fig.align = "center"}
geom_qq_line <- function(mapping = NULL,
                         data = NULL,
                         geom = "path",
                         position = "identity",
                         ...,
                         distribution = stats::qnorm,
                         dparams = list(),
                         line.p = c(.25, .75),
                         fullrange = FALSE,
                         na.rm = FALSE,
                         show.legend = NA,
                         inherit.aes = TRUE) {
  layer(
    data = data,
    mapping = mapping,
    stat = StatQqLine,
    geom = geom,
    position = position,
    show.legend = show.legend,
    inherit.aes = inherit.aes,
    params = list(
      distribution = distribution,
      dparams = dparams,
      na.rm = na.rm,
      line.p = line.p,
      fullrange = fullrange,
      ...
    )
  )
}

StatQqLine <- ggproto("StatQqLine", Stat,
 default_aes = aes(x = stat(x), y = stat(y)),
 required_aes = c("sample"),
 compute_group = function(data,
                          scales,
                          quantiles = NULL,
                          distribution = stats::qnorm,
                          dparams = list(),
                          na.rm = FALSE,
                          line.p = c(.25, .75),
                          fullrange = FALSE) {

   sample <- sort(data$sample)
   n <- length(sample)

   # Compute theoretical quantiles
   if (is.null(quantiles)) {
     quantiles <- stats::ppoints(n)
   } else {
     stopifnot(length(quantiles) == n)
   }

   theoretical <- do.call(
     distribution,
     c(list(p = quote(quantiles)), dparams)
   )

   if (length(line.p) != 2) {
     stop(
       "Cannot fit line quantiles ", line.p,
       ". Parameter line.p must have length 2.",
       call. = FALSE)
   }

   x_coords <- do.call(distribution, c(list(p = line.p), dparams))
   y_coords <- quantile(sample, line.p)
   slope <- diff(y_coords) / diff(x_coords)
   intercept <- y_coords[1L] - slope * x_coords[1L]

   if (fullrange & !is.null(scales$x$dimension)) {
     x <- scales$x$dimension()
   } else {
     x <- range(theoretical)
   }

   data.frame(x = x, y = slope * x + intercept)
 }
)

ggplot(data = data.frame(y = montosOrdenados), aes(sample = y)) + 
  geom_qq(distribution = qexp, dparams = list(rate = lambda_exp), color = moradito) + geom_qq_line(distribution = qexp, dparams = list(rate = lambda_exp)) + xlab(TeX("Cuantiles $exp(\\lambda_{exp})$")) + ylab(TeX("Montos")) + ggtitle("Q-Q Plot") + theme
```


\subsection{2.8 Estimacion para la frecuencia}

```{r estimacion frecuencia, echo=FALSE, warning=FALSE}
# Ocurrencias en cada año
ocurrencias <- seq(0, 0, length.out = floor(max(tiempoEnAnios)))
for (j in 1:length(tiempoEnAnios)) {
  tiempo <- tiempoEnAnios[j]
  anio <- floor(tiempo)
  ocurrencias[anio] <- ocurrencias[anio] + 1
}
tasa <- sum(ocurrencias)/length(ocurrencias)
desviacion <- 1.96*sqrt(tasa/10000) 
liminferior <- tasa-desviacion
limsuperior <- tasa+desviacion
```
Suponiendo que la frecuencia de los siniestros se distribuye Poisson, entonces, estimando por máxima verocimilitud se obtiene que, en promedio, ocurren 
$$\widehat{\lambda}_{MV}=\frac{1}{T}\sum_{j=0}^{T}N_i=\frac{1}{`r floor(max(tiempoEnAnios))`}\sum_{j=0}^{`r floor(max(tiempoEnAnios))`}N_i=`r tasa` $$
siniestros al año. En donde $T$ es el total de años sobre los cuales se tiene información y $N_j$ es el número de siniestros que ocurrieron en el $j$-ésimo año. Además, dados estos valores se tiene que un intervalo al $95\%$ de confianza para el parámetro de esta distribución Poisson es el siguiente

$$\widehat{\lambda}\pm 1.96 \sqrt{\dfrac{\widehat{\lambda}}{n}}=`r tasa`\pm `r desviacion` =(`r liminferior`,`r limsuperior`).$$

\section{Estimación con censura y truncamiento}

\subsection{3.1 Efectos de un deducible y un límite}

Sobre los 10000 siniestros simulados se considera que aplicarán un deducible de $5,000$ y un límite de $30,000$. El archivo de indemnizaciones tendrá, entonces, el siguiente aspecto:
```{r montos simulados, echo=FALSE, warning=FALSE}
##TRUNCAMIENTO
Indem<-montos
for (i in 1:10000) {
  if (Indem[i] > 30000) {
    Indem[i] = 30000
  } else if (Indem[i] < 5000) {
    Indem[i] = 0
  }
}
Indemnizacion<-as.data.frame(Indem)
#Indem[1:100]
Indem1<-subset(Indem,Indem !=0)

limite <- 30000
deducible <- 5000
fechasModif <- c()
montosModif <- c()
fechas <- archivo[[1]]
montos <- archivo[[2]]
for (i in 1:length(montos)) {
  if (montos[i] > limite) {
    montosModif <- c(montosModif, limite)
    fechasModif <- c(fechasModif, as.character(fechas[i]))
  } else if (montos[i] > deducible) {
    montosModif <- c(montosModif, montos[i])
    fechasModif <- c(fechasModif, as.character(fechas[i]))
  }
}
archivoModif <- data.frame(fechasModif, montosModif)
eliminados <- (1-(length(archivoModif[[1]])/length(archivo[[1]])))*100
eliminadosTeoricos <- pexp(deducible, rate = lambda_exp)*100
names(archivoModif) <- c("Fechas","Montos")
primerasModif <- head(archivoModif, 5)
kable(primerasModif)
```

$$\vdots$$
```{r cola archivo modif, echo=FALSE, warning=FALSE}
ultimasModif <- tail(archivoModif, 5)
chacadaModif <- data.frame(ultimasModif$Fechas, ultimasModif$Montos)
names(chacadaModif) <- c("Fechas", "Montos")
kable(chacadaModif)
```


Como resultado de imponer un deducible y un límite, se ha eliminado un  $`r eliminados`\%$ de registros en el archivo de indemnizaciones. Se aceptó la hipótesis de que los montos de los siniestros provienen de una distribución exponencial de media $`r denom`$, entonces se puede estimar de manera anaítica la proporción de registros eliminados mediante la probabilidad de que el monto de un siniestro se encuentre por debajo del deducible de $`r deducible`$. Esta probabilidad es la siguiente:
$$ Pr\{M\leq `r deducible`\}=1-e^{-\frac{`r deducible`}{`r denom`}}=`r eliminadosTeoricos`.$$

\pagebreak

La nueva seríe de tiempo generada por el proceso de riesgo es la siguiente:

```{r, serie de tiempo con deducible y limite, echo=FALSE, fig.height = 12, fig.width = 10}
# Ploting time series
# https://www.neonscience.org/dc-time-series-plot-ggplot-r
theme <- theme(
  # Leyend position
  legend.position = c(1, 0),
  legend.justification = c("right", "bottom"),
  legend.box.just = "right",
  # Legend boxes
  legend.key = element_rect(colour = 'black', fill = 'transparent', size = .5, linetype=0),
  legend.key.width = unit(1.5, "lines"),
  legend.background = element_rect(fill="transparent"),
  # Center plot title
  plot.title = element_text(hjust = 0.5),
  legend.text=element_text(size=8),
  axis.text.x=element_text(size=14)
)
fechonsias <- archivoModif[[1]]
funciononsia <- function(x) {
  value <- NA
  if (match(x,fechonsias) %% 500 !=0) {
    value <- NA
  } else {
    fechaSeparada <- strsplit(as.character(x), " ")
    value <- fechaSeparada[[1]][1]
  }
  value
}
fechunias <- sapply(fechonsias, funciononsia)
montunios <- archivoModif[[2]]
dfonsio <- data.frame(fechunias, montunios)
names(dfonsio) <- c("fechonson", "montonson")

ggplot(data = dfonsio, aes(x = c(1:length(dfonsio$montonson)), y = montonson, group = 1)) + 
geom_bar(stat="identity", color = "#00AFBB", size = .1, alpha=.1, na.rm = TRUE) +
geom_point(size=.005, color="blue", alpha=0.5) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5)) + 
scale_x_discrete(limits=dfonsio[[1]], na.translate = FALSE) + 
xlab("Fecha") + 
ylab("Monto") + 
geom_hline(yintercept=10000, size=1, color="cyan") + 
coord_flip() +
scale_y_reverse(limits=c(85000,0)) +
theme
```

La línea azul claro indica el valor de la media en exceso con un umbral de $`r deducible`$.

\subsection{3.2 Estimación para montos con deducible y límite}
Primero vamos a definir la función con el deducible de $5,000$ y un límite de $30,000$
$$
y(x) = \begin{cases}
  0 & x\leq 5,000\\
  x-5000 & 5,000\leq x \leq 30,000\\
  25,000 & 30,000\leq x
\end{cases}
$$

$$F_{Y}(y) = \begin{cases}
 0 & y = 0\\
 \frac{F_{x}(y + 5000) - F_{x}(5000)}{1-F_{x}(5000)}& 0\leq y \leq 900\\
  1  & 900\leq y
\end{cases}$$


$$f_{Y}(y) = \begin{cases}
 \frac{f_{x}(y + 5000)}{S_{X}(5000)} & 0\leq y \leq 25,000\\
 \frac{1- F_{x}(25000)}{1-F_{x}(5000)} & y = 25,000
\end{cases}$$

Un vez teniendo bien definidas funciones de Distribución y Densidad, podemos hacer la función de Máxima Verosimilitud.

$$L(\lambda) =   \left(\frac{1- F_{x}(25000)}{1- F_{x}(5000)}\right)^a \cdot\prod_{i=1}^b \frac{f_{x}(y_{i} + 5000)}{1-F_{x(5000)}}$$

Ahora, sustituyendo los valores de la función definida, se tiene que:
$$L(\lambda) = \left(\frac{e^{-\lambda(25000)}}{e^{-\lambda(5000)}}\right)^a\cdot\prod_{i=1}^b \frac{\lambda e^{-\lambda(y_{i} + 5000)}}{e^{-\lambda(5000)}}$$
sacando Logaritmo a la función:
$$\ln(\lambda) = a(-\lambda(25000) + \lambda(5000)) + \sum_{i=1}^b \left( \ln(\lambda)-\lambda(y_{i} + 5000) + \lambda(5000)a\right)$$  
Agrupando los terminos comunes llegamos a la función a maximizar
$$l(\lambda) = a(-\lambda(2000)) +  b(\ln(\lambda)) - \sum_{i=1}^by_{i}$$


```{r estimacion de lambda exponencial, echo=FALSE, warning=FALSE}

Indem2<-subset(Indem,Indem == 25000)
Indem3<-subset(Indem1,Indem1!= 25000)
Indem25<-length(Indem2)
Indemsin25<-length(Indem3)

Flog<-function(x){
 length(Indem2)*(-x*20000)+length(Indem3)*logb(x)- x*sum(Indem3)
}

##Resolviendo con Solver en Excel, tenemos que:
lambdaMV = .0000765113632847696
mediaEst = 1/lambdaMV
mediaDatos= mean(Indem1)

##Podemos observar que la media de los datos es mayor que la media estimada ya que la segunda considera la probabilidad de los siniestros menores al deducible

##La proporción de siniestros pagados calculada analíticamente es:
EstimAnalitica<-exp(-5000*lambdaMV)*10000

```

Después de resolver con Solver en Excel, se obtuvo que el parámetro de la distribución exponencial estimado por máxima verosimilitud es $.000076511$ y su media $13,069.95$. Comparada con la media que se obtuvo de los datos, que fue de $13,734.35$, se puede notar que el parámetro estimado considera los efectos de censura y truncamiento adecuadamente.

\subsection{3.3 El percentil $95$ de la severidad}

3.3 Construye un intervalo de confianza para el percentil 95 de la severidad. Compara este intervalo con el equivalente en la pregunta 2 y comenta.
```{r IC Severidad, echo=FALSE, warning=FALSE} 

```


